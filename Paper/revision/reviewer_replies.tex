\documentclass[11pt,final]{article}
\renewcommand*\familydefault{\sfdefault}
%\usepackage{amssymb,amsmath,amsfonts,comment}
%\usepackage{amsmath,amssymb,graphicx,subfigure,psfrag}
\usepackage{amsmath,amssymb,graphicx,subfigure,psfrag,upgreek}
\usepackage{algorithm,algorithmic}
\usepackage{amssymb,mathrsfs}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{color,pdfcolmk}
\newcommand{\todo}[1]{\noindent\emph{\textcolor{red}{Todo: #1\:}}}
%\newcommand{\alennote}[1]{\noindent\emph{\vspace{1ex}\textcolor{cyan}{Alen: #1\:}}\\[1ex]}
\newcommand{\referee}[1]{\vspace{.1ex}\noindent{\textcolor{blue}{#1}}}



\begin{document}

%We thank the reviewers for their careful reading of our article 
%and the helpful comments and suggestions.
%Please find below point-by-point replies (in black) to your comments and
%questions (which are reprinted in blue). To give you an overview of all the
%changes in the paper, we also provide a diff-document that highlights the
%changes between the initial submission and this re-submission.\\[1ex]
\begin{center}
{\bf Summary of Modifications to JOMP-D-18-00403}\\[6pt]
{\bf Sensitivity-driven Adaptive Construction of Reduced-space Surrogates}\\[6pt]
By \\
Manav Vohra, Alen Alexanderian, Cosmin Safta, Sankaran Mahadevan 
\end{center}

\baselineskip=22pt


\vspace*{1in}

We thank the reviewers for their assessment of our manuscript. Please find
below point-by-point replies (in black) to your comments and questions
(reprinted in blue). Where possible, key modifications have been highlighted
in blue in the revised manuscript. We sincerely hope that with these
modifications, the paper is found suitable for publication in the
{\it Journal of Scientific Computing}.

\clearpage


\section{Replies to reviewer \#1}
\referee{This draft proposed a systematic approach for surrogate model construction in
reduced input parameter spaces in order to reduce the number of model
evaluations. The key idea is to approximate the screening parameter iteratively
in order to identify the unimportant inputs. The screening procedure also
integrates with the adaptive construction of a surrogate in the reduced space
to improve the efficiency of the methods. Several numerical examples are used
to demonstrate the efficiency and accuracy of the proposed framework.}

Comments and Questions:
\begin{enumerate}

\item \referee{P16, Figure.2,  the estimated screening parameters eq (5) based on eq(1)
quickly converge with a small number of samples (5-10 samples). It seems that
if $\mu_i$ is based on eq(1), it will still require a reasonable amount of
samples, can authors comment on why this example converges with a small number
of samples? The same question applies to Figure 5 and Figure 7.}

Convergence behavior in this case depends upon the variability in the gradient of
$G(\theta)$ with respect to individual parameters, $\theta_i$ in their respective
domains. For the numerical tests and the application considered in this work, the
gradient is not observed to vary significantly in the considered domain of the
parameters. In other words, the variance of the estimator is low. 
Hence, a small number of Monte Carlo samples are able to estimate
the screening parameters. This is a common observation in many practical
applications, and therefore the proposed methodology involving an iterative
approach can yield significant computational savings. 

These points have been added in the discussion in Section~6 (Summary and
Conclusion, Page 34) in the revised manuscript. 

\item 
\referee{P17, Figure 3, what's the degree of the PCE used in the example? The same
question applies to Figure 6 and Figure 8.}

Sparse PCEs were constructed using least angle regression by means of UQLab software
for the two numerical tests (Borehole and the semilinear elliptic PDE),
and the reaction kinetics application in this work.  
In the borehole test case, the PCE constructed using 50 training points for the 7D, 5D, and
4D cases was found to be of degree 2, 2, and 3 respectively.  
In the semilinear elliptic PDE example, the PCE constructed using 90 training
points for the 10D and 5D cases was found to be of degree 2 and 3 respectively.
For the reaction kinetics application, the PCE constructed using 60 training points
for the 19D and 4D cases was found to be of degree 1 and 3 respectively. Note that
in each case, the degree refers to the maximal degree associated with the polynomial
basis as reported by UQLab. 

The captions of Figures 3, 6, and 8 have been updated to include this information. 

\item
\referee{
P25, Figure 7, it seems that the rank of the parameters and the difference of
$\mu_s$ doesn't change too much over iterations. Does it indicate the initial
parameter screening  (line 1-6 in Algorithm 1) is good enough for ranking the
parameters? Can the authors plot the similar figure for other examples to see
if the iterative screening procedure does make a difference on the ranking the
parameters compared with the initial parameter screening step?}

The reviewer is correct in pointing out from Figure 7 that the initial screening step
reasonably approximates the screening metric. However, as shown in Algorithm~1,
the screening procedure involves atleast $s_\text{min}$ iterations to ensure that
the screening metric has converged. The same is observed in
Figure 5 for the semilinear elliptic PDE test case. It is also true in the borehole test
case and a similar figure has been included for this example in the revised manuscript
(Figure 2c) as suggested by the reviewer. The reason for this observation has already
been discussed in response to Comment~1 above and included in the revised manuscript. 

\end{enumerate}

\section{Replies to reviewer \#2}

\referee{
An adaptive surrogate model construction approach in reduced input parameter
space is proposed in this paper. Instead of variance based sensitivity indices,
derivative based global sensitivity measures are employed to calibration of the
important model inputs, upon which surrogate models are constructed. To
demonstrate the computational advantage, the proposed approach is then
validated against three examples.  The reviewer’s recommendation is ``major
revision''.}

Major comments:
\begin{enumerate}

\item
\referee{
In this paper, the constructed surrogate model is either a RSS or a FSS. Based
on the diagram on page 12, the resulting RSS is built upon the not sufficiently
accurate FSS model (which could lead the accuracy to decrease further), or
is the RSS constructed directly from the original model with reduced input
parameters? The diagram is ambigous on this issue.}

The reviewer correctly points out that the resulting surrogate model is either an RSS or
an FSS. However, we would like to clarify that the main purpose of the proposed
methodology is to construct an RSS based on estimates of a screening metric that
involves derivative-based global sensitivity measure (DGSM) for each uncertain parameter.
However, since estimating the metric requires model evaluations, we further exploit these
evaluations to construct an FSS (free of cost) using a sparse regression-based approach
as mentioned in the paper. Note that the FSS construction is performed simultaneously to 
ensure that the computational effort associated with obtaining converged estimates of
the screening metric and RSS construction does not overshoot that required for
constructing a sufficiently accurate FSS. 

The above points are already included in the paragraph on the discussion of the proposed
methodology in Section 3. Nevertheless, based on the reviewer's feedback, we have tried to
clarify and emphasize these points in the first paragraph in Section 6 (Summary and Conclusion)
in the revised manuscript. 

\item
\referee{The reviewer would also like to see the comparison of the computational cost
(not just the numerical results) of DGSMs to variance based sensitivity indices
with increasing parameter dimension, e.g. Sobol’ indices incorporated with
sparse grid, since the difference decreases with increasing number of parameters.}

As suggested by the reviewer, the computational cost pertaining to the estimation of DGSM-based
sensitivity indices using the approach presented in this work is compared with the cost associated 
with estimating the total-effect Sobol' indices using a sparse-basis PCE
for the 19-dimensional reaction kinetics application. Additionally, the comparison is 
performed for the same application with 33 uncertain rate-controlling parameters to investigate
the impact of dimensionality. 

For the 19-dimensional case, the DGSM-based approach seems 
slightly more efficient depending upon the model runs required for training and verifying the sparse-basis PCE
used to compute the total-effect Sobol' indices. However, computational gains are observed to 
be much more substantial in the case of DGSM-based approach for the 33-dimensional problem
owing to the increase in the number of training points required to construct the PCE. Our analysis
clearly indicates that with increasing dimensionality, the DGSM-based approach presented in this
work is expected to yield a higher computational dividend especially when efficient gradient estimation
techniques are employed. 

A new sub-section (5.3) on ``comparative cost analysis" comprising the results and
discussion based on the above points has been included in the revised
manuscript. 

\item 
\referee{In section 4 and 5, no predictive convergence using the test data sample is
presented, only the convergence of $\epsilon_\text{LOO}$, which reuses the training data as
the validation set, is shown. This shows only the convergence of the model,
and does not necessarily indicate the (predictive) accuracy of the constructed
model, hence could lead to false conclusion. The reviewer would like to see the
convergence of $\epsilon_{L_2}$.}

As suggested by the reviewer, the plots and related discussion pertaining to the convergence 
of $\epsilon_{L_2}$ have been included for the numerical tests presented in Section 3 (Figures 3b
and 6b) and 
the chemical kinetics application presented in Section 4 (Figure 8, right). As expected, the predictive
accuracy of the surrogates is observed to increase with the number of training points in all cases. 
Additionally, the RSS is observed to be more accurate when a small number of training samples, $N$=10
is used. However, as $N$ increases the FSS is observed to be more accurate depending upon the
application. 

\item 
\referee{How would the proposed approach behave when presented with severe nonlinearity
or rare events where the screening metrics could misrepresent the significance
of parameters?}

Severe non-linearity in the model output as a function of uncertain inputs could potentially lead
to a significantly large variability in its gradient. Hence, a large number of Monte Carlo samples might be
required to estimate the screening metrics with reasonable accuracy in that case depending upon the
application. Therefore, the computational effort is expected to increase in such cases. In the presence
of rare events, the variance-based approaches typically lead to inaccurate estimates of 
parametric sensitivities. Since the proposed methodology in this work aims to reduce the computational
effort pertaining to total-effect Sobol' index which is a variance-based measure, it might lead to
inaccurate estimates of the screening metric and therefore to erroneous relative importance of the uncertain
inputs.

A brief discussion based on the above points has been included in the last paragraph in
Section 6 (Summary and Discussion).

\item 
\referee{The reviewer would like to see some comments on the change of computational
cost of the proposed approach with respect to the increase of parameter dimensions.}

As mentioned earlier in the response to Comment $\#$ 2, a comparative analysis of the
computational cost with increasing parameter dimensions has been included for the reaction
kinetics application in Section 5.3 in the revised manuscript. Additional comments pertaining
to the increase in computational cost and expected gains with dimensionality, based on
our findings have been included in Section 6 (Summary and Discussion).

\end{enumerate}

Minor comments have also been addressed in the revised manuscript. The authors thank
the reviewer for pointing those out as well. 

\end{document}
