\section{Background}
\label{sec:bg}

In this section, we introduce the notations used in the rest of
the article, and present the requisite background material on 
derivative-based global sensitivity measures and surrogate modeling 
using polynomial chaos expansion.




\subsection{Derivative-based global sensitivity analysis}  

Let $\GG$ be a mathematical model that is a function of $\Np$ uncertain 
parameters, $\theta_1, \theta_2, \ldots, \theta_\Np$. The goal of sensitivity analysis
is measuring the influence of each component of the parameter vector 
$\bm{\theta}$ on the model output. 
In the present work, we consider the case where the input parameters are statistically 
independent. 

Derivative-based global sensitivity analysis (DGSA) is performed by 
computing derivative based global sensitivity measures (DGSMs)~\cite{Sobol:2009} 
for each uncertain parameter in the model. 
Specifically, we consider the following DGSMs, 
\be
\mu_i = 
\mathbb{E}\left[\left(\frac{\partial \GG(\bm{\bm{\theta}})}{\partial \theta_i}\right)^{2}\right], \quad i = 1, \ldots, \Np.
\label{eq:mu}
\ee
Here $\mathbb{E}$ denotes expectation over the uncertain parameters.
Notice that this formulation assumes that the function $\GG$ is differentiable
with respect to $\theta_i$, $i = 1, \ldots, \Np$, almost surely.

If an analytic expression for $\GG$ is available the derivative in the above
expression can be computed directly. In applications, however, $\GG$ is often
defined in terms of a solution of a mathematical model. In some cases, 
one has access to appropriate adjoint solvers that enable efficient
adjoint-based gradient computations. In the present work, we consider
a generic computational model and only assume that the model
output depends differentiablly to the parameter $\bm{\theta}$. Thus, 
we consider finite-difference gradient computation: 
\be
\frac{\partial \GG(\bm{\theta})}{\partial \theta_i} 
\approx
\frac{\GG(\theta_1,\ldots,\theta_{i-1},
\theta_i+\Delta\theta_i,
\theta_{i+1},\ldots,\theta_d) - 
\GG(\bm{\theta})}{\Delta\theta_i}, \quad i = 1, \ldots, \Np. 
\label{eq:partial}
\ee
Then,~\eqref{eq:mu} can be evaluated by Monte Carlo sampling in
the uncertain parameter space. 
The total number of model realizations or function evaluations
needed to
compute $\mu_i$ for a function $G$ of $\Np$ random inputs and using $N$ samples is
therefore, $N\times(\Np+1)$. 
%The derivative in Eq.~\ref{eq:partial} could be evaluated
%analytically in case the functional dependence of $\GG$ and $\theta_i$
%is known. 
%Note that the perturbation $\Delta\theta_i^{*}$ is extremely small 
%compared to the uncertainty in $\theta_i$ which leads to a significant improvement
%over the `elementary effect' of a parameter, estimated in the Morris method~\cite{Morris:1991}
%using large increments.  
It is noted by many authors~[REFS], and also observed
in the numerical experiments in the present work, that a modest Monte Carlo
sample size is often sufficient for computing~\eqref{eq:mu} with reasonable
accuracy.

Consider the total 
Sobol' sensitivity index~\cite{Sobol:2001},
\be
\mathcal{T}(\theta_i) = 1 - 
\frac{\mathbb{V}[\mathbb{E}(\GG|\bm{\theta}_{\sim i})]}{\mathbb{V}(\GG)},
\label{eq:total}
\ee
where $\bm{\theta}_{\sim i}$ is the random vector $\bm\theta$ with $i$th component removed, 
and $\mathbb{V}$ denotes the variance. The total Sobol' index quantifies the total contribution 
of $\theta_i$ to variance of the model $\GG$. Components of $\bm\theta$ with small 
total Sobol' index can be considered inessential and can be fixed at nominal values. However, 
computing the total Sobol' index is a computationally expensive task for expensive-to-evaluate 
models with large number of uncertain parameters. Fortunately, 
for parameters with continuous distributions, an upper bound on $\mathcal{T}_i$  
can be expressed in terms of $\mu_i$, the Poincar\'e constant ($\mathcal{C}_i$) and the total 
variance of the model output ($\mathbb{V}(\GG)$)~\cite{Lamboni:2013}:
\be
\mathcal{T}(\theta_i) \leq \frac{\mathcal{C}_i\mu_i}{\mathbb{V}(\GG)}~(\propto \widehat{\mathcal{C}_i\mu_i})
\label{eq:bound}
\ee

\noindent The upper bound in the above inequality is proportional to the product of $\mathcal{C}_i$
and $\mu_i$. However, for the purpose of parameter screening as discussed later in
Section~\ref{sec:method}, we consider a normalized product, $\widehat{\mathcal{C}_i\mu_i}$:

\be
\widehat{\mathcal{C}_i\mu_i} = \frac{\mathcal{C}_i\mu_i}{\sum_i \mathcal{C}_i\mu_i}
\label{eq:cmu}
\ee

\noindent The Poincar\'e constant, $\mathcal{C}_i$ is specific to the probability distribution of $\theta_i$.
Table~\ref{tab:poincare} provides its value in the case of uniform and normal
distributions.
\bigskip

\begin{table}[htbp]
\renewcommand*{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Distribution & $\mathcal{C}_i$ \\ \hline \hline 
Uniform, $\mathcal{U}[a, b]$ & $(b-a)^{2}/\pi^2$ \\ 
Normal, $\mathcal{N}(\mu,\sigma^2)$ & $\sigma^2$ \\ 
\hline
\end{tabular}
\end{center}

\caption{Poincare constant for the case of uniformly and normally distributed random
parameters~\cite{Roustant:2014}.}
\label{tab:poincare}
\end{table}

\subsection{Polynomial chaos expansion}

We consider models with $\Np$ random inputs, 
$\theta_1, \ldots, \theta_\Np$ that are modeled
as statistically independent random variables. The 
variables $\theta_i$ will take in physically meaningful
ranges; it is common to parameterize input uncertainties
with canonical random variables $\xi_1, \ldots, \xi_\Np$,
which can be then shifted and scaled to obtain the corresponding $\theta_i's$.
Typical choices for distribution of $\xi_i$ include standard normal 
and uniform distribution on the interval $[-1, 1]$.
Let 
\[
   \bm{f}(\bm{x}) = \prod_{i=1}^\Np f_i(x_i), \quad \bm{x} \in \mathbb{R}^\Np
\]
where $f_i$ are probability density functions of $\xi_i$, $i = 1, \ldots, \Np$.
We consider a square integrable random variable $\GG:\R^\Np \to \R$; 
that is,
\[
\int_{\mathcal{D}} \GG(\bm{\xi})^2 \, \bm{f}(\bm{\xi})d\bm{\xi} < \infty,
\]
where $\mathcal{D}$ is the support of the distribution law of the random vector
$\bm{\xi}$. 


% for representing the
%dependence of a random observable ($\GG$) on independent uncertain or
%stochastic model parameters ($\bm{\theta}$).  Considering a joint probability
%distribution of the components of ($\bm{\theta}$) as
%$\mathbb{P}(\bm{\theta})$, the following condition is imposed on
%$\GG$:
%\be
%\mathbb{E}[\GG^2] = \int_{\mathcal{D}_{\bm{\theta}}} \GG^2 \mathbb{P}(\bm{\theta}) 
%d\bm{\theta} < \infty
%\ee

%\noindent where $\mathcal{D}_{\bm{\theta}}$ is the domain of the input parameter space. 

As mentioned in Section~\ref{sec:intro}, the 
polynomial chaos expansion
(PCE) is a commonly used tool for surrogate modeling. 
The PCE of
$\GG$ is a mean-square 
convergent series expansion~\cite{Xiu:2002,Ghanem:2003,Olivier:2010} of the form:
\be
\GG(\bm\xi) = \sum_{k=0}^\infty c_k\Psi_k(\bm{\xi}),
\ee
where $\Psi_k$'s form a multivariate orthogonal polynomial
basis---orthogonal with respect to the joint probability distribution of $\bm{\xi}$.
%
In practice, a truncated expansion is used.  Moreover, in applications, $\GG$
is a mathematical model of interest that takes a parameter vector $\bm{\theta}$
(with components in physically meaningful ranges) as input. Therefore, we 
write the truncated PC representation of a model $\GG$ as follows:
\be 
\GG(\bm\theta) \approx \GG^{\mbox{\tiny PC}}(\bm\theta) := \sum_{k=0}^{\Npc}
c_k\Psi_k (\bm\xi(\bm\theta)), 
\ee
where $\bm\xi(\bm\theta)$ is found by a simple linear transformation.

Computational strategies available for estimating the PC coefficients
($c_k$'s) typically involve techniques based on projection or regression.
Projection-based methods consider the orthogonal projection of 
$\GG$ on the PC basis $\{\Psi_k\}_{k=0}^\Npc$ and compute
the resulting expansion coefficients via quadrature~\cite{Olivier:2010}.
%on numerical quadrature for estimating the
%following expectation:
%\be
%\bm{c} = \mathbb{E}[\Psi_\alpha(\bm{\xi})\cdot\GG^{\mbox{\tiny{M}}}]
%\approx
%\sum_{i=1}^{N} \GG^{\mbox{\tiny{M}}}(\bm{\theta}^{(i)})w^{(i)}\Psi_\alpha(\bm{\xi}^{(i)})
%\ee
%\noindent where $w^{(i)}$ denotes the weight associated with the quadrature node $i$. 
Regression-based methods such as least angle regression (LAR)~\cite{Efron:2004} and least absolute shrinkage
and selection operator (LASSO)~\cite{Tibshirani:1996} aim to construct a sparse PCE~\cite{Blatman:2008}
by solving a regularized optimization problem:
\be
\hat{\bm{c}} = \mbox{argmin}~\mathbb{E}_{\bm\theta}
\left[\left(\sum_{k=0}^\Npc c_k \Psi_k(\bm\xi(\bm\theta)) -
\GG^{\mbox{\tiny{M}}}(\bm{\theta})\right)^{2}\right]  + \lambda\normone{\bm{c}}
\ee
where $\normone{\bm{c}}$ = $\sum_{k=0}^\Npc |c_k |$.
The regularization term forces the minimization towards sparse coefficient vectors resulting
in sparse PC representations.
In this work, we construct sparse PCEs with LAR using UQLab~\cite{Marelli:2014},
a general purpose uncertainty quantification software developed at ETH Zurich.
% in Switzerland.


















