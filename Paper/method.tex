\section{Methodology}
\label{sec:method}

%Stage1: QoI selection for accurate estimation of derivatives and PCE, Stage 2: Parameter Screening, Stage 3: Reduced-order Surrogate Verification
%
%1. Goal is to estimate the upper bound on Sobol total-effect index using DGSM. 
%2. QoI should be differentiable w.r.t all the parameters. 
%3. Derivative could be estimated analytically or numerically.
%4. The expected value of the partial derivative w.r.t a given parameter is approximated over a few samples.
%5. Gradually enhance the sample size until some degree of convergence is established.
%6. Note that model runs at the full parameter space will be used for verification of the ROS. However, one could consider
%relaxing the convergence criterion to reduce computational costs. 
%7. Consider the product of Ci and mu_i to screen parameters.i

In this section, we outline the underlying framework for \emph{adaptively}
constructing a reduced-order surrogate (ROS) using sensitivity analysis.  The
proposed methodology is described as adaptive since the ROS is constructed only
in situations where it is expected to yield computational dividend as discussed
further below.  The term reduced-order implies that the surrogate is
constructed in a subspace that sufficiently captures the uncertainty in the
model output. We begin by outlining an algorithm for parameter screening
to assess the importance of individual parameters for potential dimension
reduction and construction of an ROS. The overall adaptive framework 
which incorporates parameter screening as an integral step is thereafter
presented. Finally, we present validation metrics used for assessing the
convergence and accuracy of the ROS followed by a brief discussion on
salient features of the proposed framework. 

\textbf{Parameter screening.}
Dimension reduction is guided by global sensitivity analysis (GSA).  A typical
approach to GSA involves estimating the Sobol sensitivity
indices~\cite{Sobol:2001} in the case of non-linear mathematical models.
However, as discussed earlier in sections~\ref{sec:intro} and~\ref{sec:bg},
determining accurate estimates of Sobol sensitivity indices typically requires
tens of thousands of model evaluations. Consequently, the exercise becomes
prohibitive for expensive-to-evaluate models.  In the proposed framework, we
adopt a novel approach for constructing an ROS based on estimating the
upper-bound $\widehat{\mathcal{C}_i\mu_i}$, given in~\eqref{eq:bound}, on 
Sobol total effect index ($\mathcal{T}(\theta_i)$) for each 
parameter, $\theta_i$, and use it as a metric to screen 
parameters that are relatively unimportant. A general
methodology for parameter screening is provided below in
Algorithm~\ref{alg:screen}.

%%%%%%%%%%% Revised %%%%%%%%%%%
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Parameter screening with DGSMs: A generalized approach.}
  \begin{algorithmic}[1]
\Require $\tau > 0$, $\tau_\text{screen} > 0$,
$s_\text{max} \geq 1$, $\beta > 0$, $\{ \bm{\theta}_k \}_{k = 1}^{n_0}$, $\{ \bm{g}^k \}_{k=1}^{N_\text{total}}$. 
\Ensure $\mathcal{I}_\text{active}$, $\{ \bm{g}^k \}_{k=1}^{N_\text{total}}$, $N_\text{total}$. 
    \Procedure{Screening}{}
    %  \State Draw samples $\{ \bm{\theta}_k \}_{k = 1}^{n_1}$ 
     %  according to the probability density $\bm{f(\theta)}$.
     %   \State $N_\text{total} = N_\text{total} + n_1$.
      \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$, 
             $k~=~N_\text{total}+1, \ldots, N_\text{total}+n_1$. 
      \State $N_\text{total} = N_\text{total} + n_1$
      \State Compute 
      $\mu_{1, i} = \frac{1}{N_\text{total}} \sum_{k = 1}^{N_\text{total}} (g^k_i)^2$
      \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{1,i}}$, for each $\theta_i$, 
             $i = 1, \ldots, \Np$. 
      \State Determine initial ranks: 
            let $\mathcal{R}^{old} = \{ \nu_{i_1}, \nu_{i_2}, \ldots, \nu_{i_\Np}\}$ such that 
\[
   \nu_{i_1} \geq \nu_{i_2} \geq \cdots \geq \nu_{i_\Np}. 
\]
      \State Set $s$ = 1.
      \State Set $\mathrm{done} = \mathrm{false}$.
      \While {$\mathrm{done} == \mathrm{false}$ \textbf{AND} $s \leq s_\text{max}$} 
        \State $s = s + 1$.
        \State Draw $n_s = \lceil \beta n_1 \rceil$ new samples 
                  $\bm{\theta}_k$, $k = n_{s-1} + 1, \ldots, n_{s-1} + n_s$
       \State $N_\text{total} = N_\text{total} + n_s$.
        \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$,
             $k = n_{s-1}+1, \ldots, n_{s-1}+n_s$.

        \State Compute $\{ \mu_{s,i} \}_{i=1}^\Np$ using the augmented sample 
               $\{\bm{g}_k \}_{k = 1}^{N_\text{total}}$.
        \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{s,i}}$, $i = 1, \ldots \Np$.
        \State Determine new ranks $\mathcal{R}^{new}$ based on $\{\nu_i\}_{i=1}^\Np$. 
        \State Compute $\displaystyle\Delta\mu_s = \max_{1\leq i \leq \Np}
               \left(\frac{|\mu_{s,i} - \mu_{s-1,i}|}{ \mu_{s-1,i}}\right)$.
      \If {$\mathcal{R}^{\tiny{new}} = \mathcal{R}^{\tiny{old}}$ {\bf AND}  $\Delta\mu_s \leq \tau$}
         \State $\mathrm{done} = \mathrm{true}$
      \Else
          \State Set $\mathcal{R}^{old} = \mathcal{R}^{new}$
      \EndIf
    \EndWhile
    \State $\mathcal{I}_\text{active} = \{ i \in \{1, \ldots, \Np\} : \displaystyle\frac{\nu_i}
        {\|\bm{\nu}\|_\infty} > \tau_\text{screen}\}.$
    
    \EndProcedure
  \end{algorithmic}
  \label{alg:screen}
\end{breakablealgorithm}
\bigskip

In situations where $\nabla_{\bm{\theta}}G_k$ cannot be evaluated analytically,
we could use finite-difference to approximate its value as shown in
Eq.~\ref{eq:partial}. However, as discussed earlier in~\ref{sub:dgsm},
estimating $\bm{\mu}$  using $M$ samples in the full parameter space requires
model evaluations at $(N_p+1)M$ samples, compared to only $M$ samples in case
the derivative is available analytically. Hence, computational effort when
using finite differences is expected to increase by a factor of $(N_p+1)$.
Another possible approach, suitable in certain cases, is the use of
adjoint-based gradient computation. In such cases, each gradient computation,
which is more efficient than finite-differences---each gradient
evaluation requires a forward model solve and adjoint solve---but 
requires the availability of adjoint solvers.
\alennote{I will put some references there ...} 

Based on the steps outlined in the above algorithm, it can be said that the set
of sample points used for estimating the screening metric,
$\widehat{\mathcal{C}_i\mu_i}$ is enriched until consistency in ranks between
successive iterations as well as a certain degree of convergence in estimates
of the screening metric for each uncertain parameter is accomplished. However,
the amount of computational effort associated the screening process is limited
by the choice of maximum number of iterations, $s_\text{max}$. 
Key inputs
are as follows: (1) 
a limiting value $\tau$ of the maximum relative change in the sensitivity
measure between succesive iterations; (2) 
a limiting ratio $\tau_\text{screen}$ of the sensitivity metric 
relative to its maximum value;
(3) a real number $\beta \in (0, 1)$ to 
guide the number of new samples $\lceil \beta n_1 \rceil$ 
at each iteration ($\lceil \beta n_1
\rceil$ 
is the smallest integer greater than or equal to $\beta n_1$); 
(4) a set of samples $\{ \bm{\theta}_k \}_{k = 1}^{n_0}$ 
for the initial screening step in
the algorithm and the corresponding gradient evaluations 
$\{ \bm{g}^k \}_{k=1}^{n_0}$, where
$\bm{g}^k = \nabla_{\bm{\theta}} G(\bm{\theta}_k)$. 
The outputs are the set of active
indices $\mathcal{I}_\text{active}$ corresponding to the 
\emph{important parameters}, the total number of available model 
evaluations $N_\text{total}$, and the enriched set of gradient evaluations 
$\{ \bm{g}^k\}_{k=1}^{N_\text{total}}$. 
Parameter screening is an integral part of the
overall surrogate model construction framework 
detailed below.
 
\begin{figure}[htbp]
\tikzset{
    arro/.style={{Square[]->}}
}  

\begin{tikzpicture}[node distance=1.5cm]

\node (start) [startstop] {Start};

\node (val) [process, below of=start, text width=14.5em] {Create a validation test suite using
pre-allocated resources.};

\draw [arro] (start) -- (val);

\node (qoi) [io, below of=val,align=left,yshift=-0.2cm] {Select an appropriate model output};

\draw [arro] (val) -- (qoi);

\node (tol) [io, below of=qoi, text width=9em,align=left,yshift=-0.2cm] {Initialize: $\tau$, $\tau_\text{screen}$, 
$s_\text{max}$, $\beta$, $N_\text{total}$ = 0};

\draw [arro] (qoi) -- (tol);

\node (sam0) [process, below of=tol, text width=12.5em, yshift=-0.3cm] {Draw $n_1$ samples $\{ \bm{\theta}_k \}_{k = 1}^{n_1}$ 
     according to $\bm{f(\theta)}$};
     
\draw [arro] (tol) -- (sam0);     

%\node (counter) [process, below of=sam0, text width=9.5em, yshift=-0.21cm] {\vspace{-6mm}\be N_\text{total} = N_\text{total} + n_0 \nonumber\ee};

%\draw [arro] (sam0) -- (counter);     

\node (fss) [process, below of=sam0, text width=14.5em, yshift=-0.5cm] {Construct regression-based 
surrogate in full-space (FSS) uing ($N_\text{total}$+$n_1$) model evaluations};

\draw [arro] (sam0) -- (fss);   

\node (chk) [process, below of=fss, text width=12.5em, yshift=-0.5cm] {Assess accuracy of FSS using the
validation test suite};

\draw [arro] (fss) -- (chk);   

\node (res) [draw, diamond, aspect=1.5, text width=7.5em, text centered, below of=chk,yshift=-1.5cm] {Is \\ FSS, 
sufficiently accurate$?$};

\draw [arro] (chk) -- (res);  

\node (screen) [process, right of=res, text width=9.5em, xshift=4.5cm] {Parameter Screening};

\draw [arro] (res) -- node[anchor=north] {N} (screen);

\node (dr) [draw, diamond, aspect=1.8, text width=7.5em, text centered, right of=screen,xshift=4cm] {Is $\alpha$
small enough$?$};

\draw [arro] (screen) -- (dr); 

\node (dtol) [io, above of=dr, text width=10.5em,align=left,yshift=3.2cm,inner sep=0.2pt] {\vspace{1mm}Update: $\tau$, 
$\tau_\text{screen}$, $s_\text{max}$, $\beta$; Input: $\{ \bm{g}^k \}_{k=1}^{N_\text{total}}$\vspace{1mm}};

\draw [arro] (dr) -- node[anchor=east,yshift=-0.8cm] {N} (dtol);

\draw [arro] (dtol) |- (sam0); 

\node (ros) [process, below of=dr, text width=11.5em, yshift=-1.5cm] {Construct a reduced-order surrogate (ROS)};

\draw [arro] (dr) -- node[anchor=east,yshift=0.1cm] {Y} (ros);

\node (ver) [process, left of=ros, text width=13.0em, xshift=-5cm] {Test ROS accuracy using
 evaluations at $N_\text{total}$ $\&$ the validation test suite};

\draw [arro] (ros) -- (ver); 

\node (stop) [startstop, below of=chk,yshift=-4.5cm] {Stop};

\draw [arro] (res) -- node[anchor=east,yshift=0.0cm] {Y} (stop);

\draw [arro] (ver) -- (stop); 

\end{tikzpicture}

\caption{Flow-diagram outlining the adaptive strategy for constructing reduced-order surrogates.}
\label{fig:flow}
\end{figure}
%\clearpage

\textbf{Adaptive surrogate model construction.}
We begin by setting aside computational resources for constructing a validation
test suite to be used for assessing the accuracy of the resulting surrogate.
Naturally, the resources allocated for this purpose depend upon the application
as well as total amount of available resources. 
The set of required inputs for parameter screening are initialized,
and model evaluations at $n_0$ random samples in the full-space are computed.
These evaluations are used to construct a surrogate in the full-space (FSS)
using regression-based techniques. If the surrogate is found to be sufficiently
accurate for the given application, the process is terminated. However, it is
likely that a full-space surrogate constructed using a 
small number of model evaluations would not provide a 
faithful representation of the input-output relationship.  

The available set of model evaluations are utlized and further enhanced during
parameter screening as discussed earlier.  At the end of screening, the set of
active indices, $\mathcal{I}_\text{active}$, is used to evaluate $\alpha$ in the
following equation: 
%
\be
\alpha = \frac{|\mathcal{I}_\text{active}|}{N_p},
\label{eq:alpha}
\ee
%
where $|\mathcal{I}_\text{active}|$ denotes the cardinality of
$\mathcal{I}_\text{active}$.  Scope for dimension-reduction increases as
$\alpha$ decreases.  Hence, if $\alpha$ is considered to be small and
computational gains are expected owing to dimension reduction, the ROS is
constructed and verified for accuracy using a combination of model evaluations
used for screening and those associated with the validation test suite.  On the
other hand, if $\alpha$ is close to 1, the set of inputs required for screening are
 updated as needed, and a
new set of $n_0$ samples and corresponding model evaluations are generated. The
FSS is re-constructed using the enriched set of evaluations and the
aforementioned analysis is repeated as illustrated in the flow-diagram
in Figure~\ref{fig:flow} that shows the overall 
parameter screening and surrogate model construction method.



\textbf{Validation metrics.}
To assess convergence of the resulting surrogate, one could estimate the
leave-one-out cross validation error as follows:
\be
\epsilon_{\mbox{\tiny LOO}} = 
\frac{\sum\limits_{i=1}^{N_l}\left(G(\bm{\theta}_i) - G^{\mbox{\tiny {PC}\textbackslash i}}(\bm{\xi(\theta}_i))\right)^2}
{\sum\limits_{i=1}^{N_l}\left(G(\bm{\theta}_i) - \widetilde{\mu}\right)^2}
\label{eq:loo}
\ee
where $N_l$ is the number of training points,
$\widetilde{\mu}~=~\frac{1}{N_l}\sum\limits_{i=1}^{N_l} G(\bm{\theta}_i)$ is
the sample mean of the model response, and $ G^{\mbox{\tiny {PC}\textbackslash
i}}$ is the PC surrogate constructed using all but the $i^{\mbox{\tiny{th}}}$
model realization.  From~\eqref{eq:loo}, it appears that $N_l$ PCEs are needed
to evaluate $\epsilon_{\mbox{\tiny LOO}}$.  However, in practice a modified
formulation for $\epsilon_{\mbox{\tiny LOO}}$~\cite{Blatman:2009}, independent
of $G^{\mbox{\tiny {PC}\textbackslash i}}$ is used; for an easy reference,
see~\cite[Eq.~(1.27)]{Marelli:2014}.  Accuracy of the surrogate could also 
be assessed by evaluating the relative $\ell_2$-norm of the difference in
predictions between the model and the surrogate ($\epsilon_{\mbox{\tiny{L-2}}}$), as follows:
\be
\epsilon_{\mbox{\tiny{L-2}}} = \frac{\left[\sum\limits_{i=1}^{N_v}\left(G(\bm{\theta}_i) - 
G^{\mbox{\tiny {PC}}}(\bm{\xi(\theta}_i))\right)^2\right]^{\frac{1}{2}}}
{\left[\sum\limits_{i=1}^{N_v}\left(G(\bm{\theta}_i)\right)^2\right]^{\frac{1}{2}}}
\label{eq:l2}
\ee
where $N_v$ is the number of sampling points in the full parameter space at
which model evaluations are available. In the case of an ROS, it is the
augmented set of model evaluations used for validation and screening.  
%
Accuracy of the surrogate could be further investigated by comparing
probability density functions (PDFs) of the model output based on model
evaluations in the full parameter space and the ROS predictions corresponding
to a large number of samples (say, 10$^6$).  However, in realistic problems
involving complex, compute-intensive simulations, the PDF based on model
evaluations would be intractable.  A practical alternative would be to compare
a histogram based on sparse model evaluations with the surrogate-based PDF in
order to gain some insight into the quality of the comparison. 

\textbf{Remarks.}
The amount of computational effort associated with the presented methodology
can be mainly attributed to two steps: I.~Parameter Screening, and 
II.~Constructing a converged ROS. Computational gains are realized in situations
where constructing the surrogate in the full parameter space is more expensive
than the combined cost associated with these steps. Determining the optimal
allocation of computational resources for these steps, however, is not possible
a priori. Hence, in the proposed framework, we exploit the set of model
evaluations used in parameter screening to simultaneously construct the FSS
while keeping a track of its accuracy using the validation test suite. This
would help address situations where significant dimension reduction is not
possible, and hence, constructing the ROS might result in a computational
disadvantage. We suggest using a small number of samples in the initial
screening step (say, $n_0$ = 5) and a relatively large $\tau$ (say,
$\mathcal{O}(10^{-1})$) as a starting point with possible reduction in $\tau$
during subsequent screenings. 


The applicability of the proposed framework depends upon the choice of the
model output.  Since the screening metric involves computation of partial
derivatives in the full parameter space, the output must exhibit differentiable
dependence on each parameter. It is therefore likely that for a given
application involving multiple outputs, the ROS can only be constructed for a
selected few, using the approach presented above.  Hence, it is important to
assess the nature of the input-output relationship for a given model prior to
implementing the present framework.

Using the framework proposed in this section, we aim to construct a reliable
surrogate in the most efficient manner within the constraints of the computational
budget. However, it is likely that for a given application, the ROS is not
found to be sufficiently accurate. In such a scenario, we suggest enriching the
set of important parameters by incorporating the least unimportant parameter
as determined after a series of screening steps, and re-constructing the ROS. 
This process could be repeated depending upon the availability of resources
and the desired accuracy of the surrogate.  


%\newpage
\iffalse
\begin{breakablealgorithm}
  \caption{Parameter screening with DGSMs: A generalized approach.}
  \begin{algorithmic}[1]
    \Procedure{Screening}{}
      \State Draw samples $\{ \bm{\theta}_k \}_{k = 1}^{n_1}$ 
       according to the probability density $\bm{f(\theta)}$.
      \State $N_t = N_t + n_1$
      \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$, 
             $k = 1, \ldots, n_1$. 

      \State Compute 
      $\mu_{1, i} = \frac{1}{N_t} \sum_{k = 1}^{N_t} (g^k_i)^2$
      \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{1,i}}$, for each $\theta_i$, 
             $i = 1, \ldots, \Np$. 
      \State Determine initial ranks: 
            let $\mathcal{R}^{old} = \{ \theta_{\sigma(i)}\}_{i=1}^{N_t}$ such that 
\[
   \nu_{\sigma(1)} \geq \nu_{\sigma(2)} \geq \cdot \geq \nu_{\sigma(\Np)}. 
\]
      \State Set $s$ = 1.
      \State Set $\mathrm{done} = \mathrm{false}$.
      \While {$\mathrm{done} == \mathrm{false}$ \textbf{AND} $s \leq s_\text{max}$} 
        \State $s = s + 1$
        \State $N_t = N_t + n_s$
        \State Draw $n_s$ new samples 
                  $\bm{\theta}_k$, $k = 1, \ldots, n_s$
        \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$
        \State Compute $\{ \mu_{s,i} \}_{i=1}^\Np$ using the augmented sample 
               $\{\bm{g}_k \}_{k = 1}^{N_t}$.
        \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{s,i}}$, $i = 1, \ldots \Np$.
        \State Determine new ranks $\mathcal{R}^{new}$ based on $\{\nu_i\}_{i=1}^\Np$. 
        \State Compute $\displaystyle\Delta\mu_s = \max_{1\leq i \leq \Np}
               \left(\frac{|\mu_{i,s} - \mu_{i,s-1}|}{ \mu_{i,s-1}}\right)$.
      \If {$\mathcal{R}^{\tiny{new}} = \mathcal{R}^{\tiny{old}}$ {\bf AND}  $\Delta\mu_s \leq \tau$}
         \State $\mathrm{done} = \mathrm{true}$
      \Else
          \State Set $\mathcal{R}^{old} = \mathcal{R}^{new}$
      \EndIf
    \EndWhile
    \State $\mathcal{I}_\text{active} = \{ i \in \{1, \ldots, \Np\} : \nu_i \ni \displaystyle\frac{\nu_i}{\max(\bm{\nu})}> \tau_\text{screen}\}.$
    
    \EndProcedure
  \end{algorithmic}
  \label{alg:screen}
\end{breakablealgorithm}
\fi

%\newpage
