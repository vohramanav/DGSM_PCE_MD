\section{Methodology}
\label{sec:method}

%Stage1: QoI selection for accurate estimation of derivatives and PCE, Stage 2: Parameter Screening, Stage 3: Reduced-order Surrogate Verification
%
%1. Goal is to estimate the upper bound on Sobol total-effect index using DGSM. 
%2. QoI should be differentiable w.r.t all the parameters. 
%3. Derivative could be estimated analytically or numerically.
%4. The expected value of the partial derivative w.r.t a given parameter is approximated over a few samples.
%5. Gradually enhance the sample size until some degree of convergence is established.
%6. Note that model runs at the full parameter space will be used for verification of the ROS. However, one could consider
%relaxing the convergence criterion to reduce computational costs. 
%7. Consider the product of Ci and mu_i to screen parameters.i

In this section, we outline the underlying methodology for constructing a 
reduced-order surrogate (ROS) using sensitivity analysis. The term `reduced-order'
in the present context implies that the surrogate is constructed in a 
subspace that sufficiently captures the uncertainty in the model output. 
In the case of PC surrogates used in this work, it implies that the dimensionality
of the polynomial basis functions is effectively reduced. 

A possible approach to 
constructing the ROS involves sensitivity analysis of the
uncertain model parameters with respect to a given output, and thereby
disregarding the uncertainty associated with parameters considered as unimportant.
For this purpose, a global sensitivity analysis based on Sobol indices could be pursued.
However, as discussed earlier in sections~\ref{sec:intro} and~\ref{sec:bg}, determining converged 
estimates of Sobol sensitivity indices typically requires tens of thousands
of model evaluations. Consequently, the exercise becomes prohibitive in
situations where the model runs are compute-intensive. Instead, we adopt a novel approach
that focuses on estimating the upper-bound
upper-bound ($\widehat{\mathcal{C}_i\mu_i}$, see Eq.~\ref{eq:bound}) on Sobol total
effect index ($\mathcal{T}(\theta_i)$) for each parameter, $\theta_i$, and using it as a 
metric to screen parameters that are relatively unimportant. Below, we provide a generalized
strategy for parameter screening. 

\clearpage
\iffalse
\begin{breakablealgorithm}
  \caption{Parameter screening with DGSMs: A generalized approach.}
  \begin{algorithmic}[1]
    \Procedure{Screening}{}
      \State Draw samples $\{ \bm{\theta}_k \}_{k = 1}^{n_1}$ 
       according to the probability density $\bm{f(\theta)}$.
      \State $N_t = N_t + n_1$
      \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$, 
             $k = 1, \ldots, n_1$. 

      \State Compute 
      $\mu_{1, i} = \frac{1}{N_t} \sum_{k = 1}^{N_t} (g^k_i)^2$
      \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{1,i}}$, for each $\theta_i$, 
             $i = 1, \ldots, \Np$. 
      \State Determine initial ranks: 
            let $\mathcal{R}^{old} = \{ \theta_{\sigma(i)}\}_{i=1}^{N_t}$ such that 
\[
   \nu_{\sigma(1)} \geq \nu_{\sigma(2)} \geq \cdot \geq \nu_{\sigma(\Np)}. 
\]
      \State Set $s$ = 1.
      \State Set $\mathrm{done} = \mathrm{false}$.
      \While {$\mathrm{done} == \mathrm{false}$ \textbf{AND} $s \leq s_\text{max}$} 
        \State $s = s + 1$
        \State $N_t = N_t + n_s$
        \State Draw $n_s$ new samples 
                  $\bm{\theta}_k$, $k = 1, \ldots, n_s$
        \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$
        \State Compute $\{ \mu_{s,i} \}_{i=1}^\Np$ using the augmented sample 
               $\{\bm{g}_k \}_{k = 1}^{N_t}$.
        \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{s,i}}$, $i = 1, \ldots \Np$.
        \State Determine new ranks $\mathcal{R}^{new}$ based on $\{\nu_i\}_{i=1}^\Np$. 
        \State Compute $\displaystyle\Delta\mu_s = \max_{1\leq i \leq \Np}
               \left(\frac{|\mu_{i,s} - \mu_{i,s-1}|}{ \mu_{i,s-1}}\right)$.
      \If {$\mathcal{R}^{\tiny{new}} = \mathcal{R}^{\tiny{old}}$ {\bf AND}  $\Delta\mu_s \leq \tau$}
         \State $\mathrm{done} = \mathrm{true}$
      \Else
          \State Set $\mathcal{R}^{old} = \mathcal{R}^{new}$
      \EndIf
    \EndWhile
    \State $\mathcal{I}_\text{active} = \{ i \in \{1, \ldots, \Np\} : \nu_i \ni \displaystyle\frac{\nu_i}{\max(\bm{\nu})}> \tau_\text{screen}\}.$
    
    \EndProcedure
  \end{algorithmic}
  \label{alg:screen}
\end{breakablealgorithm}
\fi


%%%%%%%%%%% 





\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Parameter screening with DGSMs: A generalized approach.}
  \begin{algorithmic}[1]
\Require $\tau > 0$, $\tau_\text{screen} > 0$,
$s_\text{max}$, $\beta > 0$. 
\Ensure $\mathcal{I}_\text{active}$, $\{ \bm{g}^k \}_{k=1}^{N_\text{total}}$. 
    \Procedure{Screening}{}
      \State Draw samples $\{ \bm{\theta}_k \}_{k = 1}^{n_1}$ 
       according to the probability density $\bm{f(\theta)}$.
      \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$, 
             $k = 1, \ldots, n_1$. 

      \State Compute 
      $\mu_{1, i} = \frac{1}{n_1} \sum_{k = 1}^{n_1} (g^k_i)^2$
      \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{1,i}}$, for each $\theta_i$, 
             $i = 1, \ldots, \Np$. 
      \State Determine initial ranks: 
            let $\mathcal{R}^{old} = \{ \nu_{i_1}, \nu_{i_2}, \ldots, \nu_{i_\Np}\}$ such that 
\[
   \nu_{i_1} \geq \nu_{i_2} \geq \cdots \geq \nu_{i_\Np}. 
\]
      \State Set $s$ = 1.
      \State Set $N_\text{total} = n_1$.
      \State Set $\mathrm{done} = \mathrm{false}$.
      \While {$\mathrm{done} == \mathrm{false}$ \textbf{AND} $s \leq s_\text{max}$} 
        \State Set $s = s + 1$.
        \State Draw $n_s = \lceil \beta n_1 \rceil$ new samples 
                  $\bm{\theta}_k$, $k = n_{s-1} + 1, \ldots, n_{s-1} + n_s$
       \State Set $N_\text{total} = N_\text{total} + n_s$.
        \State Compute $\bm{g}^k = \nabla_{\bm{\theta}}G(\bm\theta_k)$,
             $k = n_{s-1}+1, \ldots, n_{s-1}+n_s$.

        \State Compute $\{ \mu_{s,i} \}_{i=1}^\Np$ using the augmented sample 
               $\{\bm{g}_k \}_{k = 1}^{N_\text{total}}$.
        \State Compute $\nu_i = \widehat{\mathcal{C}_i\mu_{s,i}}$, $i = 1, \ldots \Np$.
        \State Determine new ranks $\mathcal{R}^{new}$ based on $\{\nu_i\}_{i=1}^\Np$. 
        \State Compute $\displaystyle\Delta\mu_s = \max_{1\leq i \leq \Np}
               \left(\frac{|\mu_{i,s} - \mu_{i,s-1}|}{ \mu_{i,s-1}}\right)$.
      \If {$\mathcal{R}^{\tiny{new}} = \mathcal{R}^{\tiny{old}}$ {\bf AND}  $\Delta\mu_s \leq \tau$}
         \State $\mathrm{done} = \mathrm{true}$
      \Else
          \State Set $\mathcal{R}^{old} = \mathcal{R}^{new}$
      \EndIf
    \EndWhile
    \State $\mathcal{I}_\text{active} = \{ i \in \{1, \ldots, \Np\} : \displaystyle\frac{\nu_i}
        {\|\bm{\nu}\|_\infty} > \tau_\text{screen}\}.$
    
    \EndProcedure
  \end{algorithmic}
  \label{alg:screen}
\end{breakablealgorithm}

\bigskip

In case the gradient, $\nabla_{\bm{\theta}}G_k$ cannot be evaluated analytically, we could use finite-difference
to approximate its value as shown in Eq.~\ref{eq:partial} in Algorithm~\ref{alg:screen}. However, as discussed earlier
in section 2.1, estimating $\bm{\mu}$  using $\mathcal{N}$ samples in the full parameter space requires 
model evaluations at model evaluations at $(N_p+1)\mathcal{N}$ samples, compared to only $\mathcal{N}$ samples in 
situations where the derivative is available analytically. Hence, computational effort when using finite differences
is increased by a factor of $(N_p+1)$. 

Based on the steps outlined in the above algorithm, it can be said that the set of sample points used for
estimating the screening metric, $\widehat{\mathcal{C}_i\mu_i}$ is enriched until consistency in ranks
between successive iterations as well as a certain degree of convergence in estimates of the screening 
metric for each uncertain parameter is accomplished. However, the amount of computational effort associated
the screening process is limited by the choice of maximum number of iterations Parameter
screening is an integral part of the overall strategy for constructing the ROS as 
depicted below using a flow-diagram. 

\bigskip

\begin{figure}[htbp]
\begin{center}

\tikzset{
    arro/.style={{Square[]->}}
}  

\begin{tikzpicture}[node distance=1.5cm]

\node (start) [startstop] {Start};

\node (qoi) [io, below of=start,align=left] {Select an appropriate model output};

\draw [arro] (start) -- (qoi);

\node (tol) [io, below of=qoi, text width=9em,align=left,yshift=-0.2cm] {Initialize: $\tau$, $\tau_\text{screen}$, 
$s_\text{max}$, $N_t$ = 0};

\draw [arro] (qoi) -- (tol);

\node (screen) [process, below of=tol, text width=9.5em] {Parameter Screening};

\draw [arro] (tol) -- (screen);

\node (output) [process, below of=screen, text width=9.5em] {Compute $\alpha$ based on $\mathcal{I}_\text{active}$};

\draw [arro] (screen) -- (output);

\node (dr) [draw, diamond, aspect=1.5,yshift=-8.4cm, text width=4.5em, text centered] {If $\alpha < 1$};

\draw [arro] (output) -- (dr);

\node (fss) [process, right of=dr, text width=9em,xshift=3.5cm] {Construct full-space surrogate (FSS)};

\draw [arro] (dr) -- node[anchor=south] {N} (fss);

\node (res) [draw, diamond, aspect=1.5, xshift=3.5cm,text width=4.5em, text centered, right of=fss] {FSS converged$?$};

\draw [arro] (fss) -- (res);

\node (dtol) [io, above of=res, text width=8.7em,align=left,yshift=2.8cm] {Update: $\tau$, $s_\text{max}$, $N_t$};
 
\draw [arro] (dtol) -| (screen);
 
 \draw [arro] (res) -- node[anchor=east,yshift=-0.6cm] {N} (dtol);
 
 \node (assess) [process, below of=dr, text width=11em,yshift=-1.2cm] {If applicable, assess $\alpha$ and convergence
  of FSS};
 
 \draw [arro] (dr) -- node[anchor=east] {Y}(assess);
 
 \node (con) [draw, diamond, aspect=1.5,text width=4em, text centered, below of=assess,yshift=-0.7cm] {Continue$?$};
 
 \draw [arro] (assess) -- (con);

\node (ros) [process, below of=con, text width=6em, yshift=-0.7cm] {Construct the ROS};

 \draw [arro] (con) -- node[anchor=east] {Y}(ros);

\node (verify) [process, below of=ros, text width=9em,yshift=-0.2cm] {Verify accuracy of the surrogate};

\draw [arro] (ros) -- (verify);

\node (stop) [startstop, right of=verify,xshift=3.5cm] {Stop};

\draw [arro] (verify) -- (stop);

\draw [arro] (res) |- node[anchor=east,yshift=6.9cm] {Y} (stop);

\draw [arro] (con) -| node[anchor=south,xshift=-3cm] {N} (stop);

\end{tikzpicture}
\end{center}

\caption{Flow-diagram outlining the adaptive strategy for constructing reduced-order surrogates using
DGSMs.}
\label{fig:flow}
\end{figure}

As illustrated in the above flow-diagram, application of the above methodology to construct an ROS 
relies on the choice of an \textit{appropriate} model output. Since the parameters are screened
using DGSMs, the model output should exhibit a smooth dependence on the
individual uncertain parameters to enable a reasonable approximation of the derivatives in Eq.~\ref{eq:mu}.
Thus, for a given problem involving multiple outputs or model predictions, it is likely that a reduced-order 
surrogate might be possible only for selected outputs. Hence, where possible, it is recommended to a priori assess the
nature of dependence of a given model output on the parameters.

The amount of computational effort associated with the presented methodology can be mainly attributed to
two steps: I. Parameter Screening, and II. Constructing a converged ROS. Computational gains are realized
in situations where constructing the surrogate in the full parameter space is more expensive than the combined cost
associated with these steps. Determining the optimal allocation of computational resources for these steps, however, 
is not possible a priori. Hence, we suggest estimating the DGSMs on a small set of samples (say, $n_1$ = 5) 
initially, and screening with a relatively large value of tolerance, $\tau$ while keeping a track of computational expense.
Tighter tolerances on $\bm{\mu}$ are
gradually imposed until the screening process yields conclusive results pertaining to the possibility of dimension-reduction
or a reasonably converged surrogate is constructed using the available set of model evaluations in the full parameter space. 
Hence, the combined computational cost associated with steps I and II could be less, equal to, or greater than that needed
to construct the surrogate in the full parameter space. To avoid the case of greater computational effort, we suggest 
that the decision to construct the ROS should account for the degree of dimension-reduction predicted at the end of the
screening step ($\alpha$, defined below in Eq.~\ref{eq:alpha}), and the state of convergence of the full-space surrogate. 
Enormous computational savings are in fact possible when dealing with complex, 
compute-intensive models, even if $\alpha$ in the following equation is
not too large. 

\be
\alpha = \frac{\mbox{Dimensionality of ROS}}{\mbox{Dimensionality of the full surrogate}}
\label{eq:alpha}
\ee

In order to assess convergence of the ROS, we estimate the leave-one-out cross validation error as
follows:

\be
\epsilon_{\mbox{\tiny LOO}} = 
\frac{\sum\limits_{i=1}^{N_l}\left(G(\bm{\theta}_i) - G^{\mbox{\tiny {PC}\textbackslash i}}(\bm{\xi(\theta}_i))\right)^2}
{\sum\limits_{i=1}^{N_l}\left(G(\bm{\theta}_i) - \widetilde{\mu}\right)^2}
\label{eq:loo}
\ee

\noindent where $N_l$ is the number of training points, 
$\widetilde{\mu}~=~\frac{1}{N}\sum\limits_{i=1}^N G(\bm{\theta}_i)$
is the sample mean of the model response, and $ G^{\mbox{\tiny {PC}\textbackslash i}}$
is the PCE surrogate constructed using all but the $i^{\mbox{\tiny{th}}}$ model realization. 
From Eq.~\ref{eq:loo}, it appears that $N$ PCEs are needed to evaluate $\epsilon_{\mbox{\tiny LOO}}$.
However, in practice a modified formulation for $\epsilon_{\mbox{\tiny LOO}}$~\cite{Blatman:2009},
independent of $G^{\mbox{\tiny {PC}\textbackslash i}}$ is used.

Finally, as outlined in Figure~\ref{fig:flow}, the converged ROS is verified for predictive accuracy. 
To this end, we compute a relative L-2 norm of the difference in predictions between the model
and the ROS ($\epsilon_{\mbox{\tiny{L-2}}}$) as follows:

\be
\epsilon_{\mbox{\tiny{L-2}}} = \frac{\left[\sum\limits_{i=1}^{N_v}\left(G(\bm{\theta}_i) - 
G^{\mbox{\tiny {PC}}}(\bm{\xi(\theta}_i))\right)^2\right]^{\frac{1}{2}}}
{\left[\sum\limits_{i=1}^{N_v}\left(G(\bm{\theta}_i)\right)^2\right]^{\frac{1}{2}}}
\label{eq:l2}
\ee

\noindent where $N_v$ is the number of sampling points in the full parameter space at which model
evaluations are available. It must be noted that the computational effort associated with estimating
 $\epsilon_{\mbox{\tiny{L-2}}}$
is negligible since we exploit the available set of model evaluations in the full parameter space, generated
during the screening step. Additionally, probability density function (PDF) based on these full-space 
model evaluations are compared with the PDF generated using a large set of evaluations of the same
output from the ROS. 


 
